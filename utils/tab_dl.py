import streamlit as st

general="""
        Deep learning is a technique used to make predictions using data, and it heavily relies on neural networks. 
        
        Deep learning framework like **TensorFlow** or **PyTorch** instead of building your own neural network. 
        That said, having some knowledge of how neural networks work is helpful because you can use it to better architect your deep learning models.

        **Traditional Machine Learning:**

        - These models typically involve feature engineering, where domain-specific features are manually crafted from raw data to feed into the learning algorithm.
        - Examples of traditional machine learning algorithms include linear regression, logistic regression, decision trees, support vector machines, and k-nearest neighbors, among others.
        - While some traditional machine learning algorithms may use ensemble techniques that combine multiple models (e.g., random forests, gradient boosting), they are not typically referred to as "multi-layer" in the same sense as deep neural networks.

        **Deep Learning:**

        Deep learning, on the other hand, specifically refers to neural networks with multiple layers (hence the term "deep").
        - Deep learning architectures consist of multiple layers of interconnected neurons, allowing them to learn complex representations and hierarchies of features directly from raw data.
        - Deep learning models are capable of automatically learning feature representations from data without requiring explicit feature engineering.
        - Examples of deep learning architectures include convolutional neural networks (CNNs) for image analysis, recurrent neural networks (RNNs) for sequential data, and transformer-based architectures for natural language processing.
        - The depth of neural networks in deep learning refers to the number of layers, and deep networks may consist of dozens or even hundreds of layers.
    """


def dl_general():
    st.image("./data/images/mlpipeline.png")
    st.markdown(general)    
    st.info("The Exchange Of Methods And Algorithms Between Human And Machine To Deep Learn And Apply Problem Solving Is Known As Deep Learning (DL) â€• P.S. Jagadeesh Kumar")


def dl_theory():
    st.header("ğŸ§ 1. Long Short-Term Memory networks")
    st.markdown("""

        LSTM networks are a type of Recurrent Neural Network (RNN) specially designed to remember and process sequences of data over long periods. 
        What sets LSTMs apart from traditional RNNs is their ability to preserve information for long durations, courtesy of their unique structure comprising three gates: the input, forget, and output gates.
        These gates collaboratively manage the flow of information, deciding what to retain and what to discard, thereby mitigating the issue of vanishing gradients â€” a common problem in standard RNNs.
    
    """)
    st.image("./data/images/lstm.png")
    
    st.header("ğŸ‘©â€ğŸ«2. Attention Mechanism")
    st.markdown("""
        The attention mechanism, initially popularized in the field of natural language processing, has found its way into various other domains, including finance. 
        It operates on a simple yet profound concept: not all parts of the input sequence are equally important. 
        By allowing the model to focus on specific parts of the input sequence while ignoring others, the attention mechanism enhances the modelâ€™s context understanding capabilities.

        Incorporating attention into LSTM networks results in a more focused and context-aware model. 
        When predicting stock prices, certain historical data points may be more relevant than others. 
        The attention mechanism empowers the LSTM to weigh these points more heavily, leading to more accurate and nuanced predictions.

        tensorflowä¸¤ç§attentionæœºåˆ¶ï¼Œåˆ†åˆ«ä¸ºBahdanau attentionï¼Œå’ŒLuongAttention.
        Attention è§£å†³äº† RNN ä¸èƒ½å¹¶è¡Œè®¡ç®—çš„é—®é¢˜ã€‚Attentionæœºåˆ¶æ¯ä¸€æ­¥è®¡ç®—ä¸ä¾èµ–äºä¸Šä¸€æ­¥çš„è®¡ç®—ç»“æœï¼Œå› æ­¤å¯ä»¥å’ŒCNNä¸€æ ·å¹¶è¡Œå¤„ç†ã€‚
        æ¨¡å‹å¤æ‚åº¦è·Ÿ CNNã€RNN ç›¸æ¯”ï¼Œå¤æ‚åº¦æ›´å°ï¼Œå‚æ•°ä¹Ÿæ›´å°‘ã€‚æ‰€ä»¥å¯¹ç®—åŠ›çš„è¦æ±‚ä¹Ÿå°±æ›´å°ã€‚
        åœ¨ Attention æœºåˆ¶å¼•å…¥ä¹‹å‰ï¼Œæœ‰ä¸€ä¸ªé—®é¢˜å¤§å®¶ä¸€ç›´å¾ˆè‹¦æ¼ï¼šé•¿è·ç¦»çš„ä¿¡æ¯ä¼šè¢«å¼±åŒ–ï¼Œå°±å¥½åƒè®°å¿†èƒ½åŠ›å¼±çš„äººï¼Œè®°ä¸ä½è¿‡å»çš„äº‹æƒ…æ˜¯ä¸€æ ·çš„ã€‚

        Attention æ˜¯æŒ‘é‡ç‚¹ï¼Œå°±ç®—æ–‡æœ¬æ¯”è¾ƒé•¿ï¼Œä¹Ÿèƒ½ä»ä¸­é—´æŠ“ä½é‡ç‚¹ï¼Œä¸ä¸¢å¤±é‡è¦çš„ä¿¡æ¯ã€‚ä¸‹å›¾çº¢è‰²çš„é¢„æœŸå°±æ˜¯è¢«æŒ‘å‡ºæ¥çš„é‡ç‚¹ã€‚

        Attention ç»å¸¸ä¼šå’Œ Encoderâ€“Decoder ä¸€èµ·è¯´ï¼Œä¹‹å‰çš„æ–‡ç« ã€Šä¸€æ–‡çœ‹æ‡‚ NLP é‡Œçš„æ¨¡å‹æ¡†æ¶ Encoder-Decoder å’Œ Seq2Seqã€‹ ä¹Ÿæåˆ°äº† Attentionã€‚
    """)
    st.image("./data/images/attention.gif")
    st.header("Attention åŸç†çš„3æ­¥åˆ†è§£ï¼š")
    st.image("./data/images/attentionpipeline.png")
    st.markdown("""

        ç¬¬ä¸€æ­¥ï¼š query å’Œ key è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼Œå¾—åˆ°æƒå€¼

        ç¬¬äºŒæ­¥ï¼šå°†æƒå€¼è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°ç›´æ¥å¯ç”¨çš„æƒé‡

        ç¬¬ä¸‰æ­¥ï¼šå°†æƒé‡å’Œ value è¿›è¡ŒåŠ æƒæ±‚å’Œ

        ä»ä¸Šé¢çš„å»ºæ¨¡ï¼Œæˆ‘ä»¬å¯ä»¥å¤§è‡´æ„Ÿå—åˆ° Attention çš„æ€è·¯ç®€å•ï¼Œå››ä¸ªå­—â€œå¸¦æƒæ±‚å’Œâ€å°±å¯ä»¥é«˜åº¦æ¦‚æ‹¬ï¼Œå¤§é“è‡³ç®€ã€‚åšä¸ªä¸å¤ªæ°å½“çš„ç±»æ¯”ï¼Œäººç±»å­¦ä¹ ä¸€é—¨æ–°è¯­è¨€åŸºæœ¬ç»å†å››ä¸ªé˜¶æ®µï¼šæ­»è®°ç¡¬èƒŒï¼ˆé€šè¿‡é˜…è¯»èƒŒè¯µå­¦ä¹ è¯­æ³•ç»ƒä¹ è¯­æ„Ÿï¼‰->æçº²æŒˆé¢†ï¼ˆç®€å•å¯¹è¯é å¬æ‡‚å¥å­ä¸­çš„å…³é”®è¯æ±‡å‡†ç¡®ç†è§£æ ¸å¿ƒæ„æ€ï¼‰->èä¼šè´¯é€šï¼ˆå¤æ‚å¯¹è¯æ‡‚å¾—ä¸Šä¸‹æ–‡æŒ‡ä»£ã€è¯­è¨€èƒŒåçš„è”ç³»ï¼Œå…·å¤‡äº†ä¸¾ä¸€åä¸‰çš„å­¦ä¹ èƒ½åŠ›ï¼‰->ç™»å³°é€ æï¼ˆæ²‰æµ¸åœ°å¤§é‡ç»ƒä¹ ï¼‰ã€‚

        è¿™ä¹Ÿå¦‚åŒattentionçš„å‘å±•è„‰ç»œï¼ŒRNN æ—¶ä»£æ˜¯æ­»è®°ç¡¬èƒŒçš„æ—¶æœŸï¼Œattention çš„æ¨¡å‹å­¦ä¼šäº†æçº²æŒˆé¢†ï¼Œè¿›åŒ–åˆ° transformerï¼Œèæ±‡è´¯é€šï¼Œå…·å¤‡ä¼˜ç§€çš„è¡¨è¾¾å­¦ä¹ èƒ½åŠ›ï¼Œå†åˆ° GPTã€BERTï¼Œé€šè¿‡å¤šä»»åŠ¡å¤§è§„æ¨¡å­¦ä¹ ç§¯ç´¯å®æˆ˜ç»éªŒï¼Œæˆ˜æ–—åŠ›çˆ†æ£šã€‚

        è¦å›ç­”ä¸ºä»€ä¹ˆ attention è¿™ä¹ˆä¼˜ç§€ï¼Ÿæ˜¯å› ä¸ºå®ƒè®©æ¨¡å‹å¼€çªäº†ï¼Œæ‡‚å¾—äº†æçº²æŒˆé¢†ï¼Œå­¦ä¼šäº†èä¼šè´¯é€šã€‚

        **Attention çš„ N ç§ç±»å‹**
        Attention æœ‰å¾ˆå¤šç§ä¸åŒçš„ç±»å‹ï¼šSoft Attentionã€Hard Attentionã€é™æ€Attentionã€åŠ¨æ€Attentionã€Self Attention ç­‰ç­‰ã€‚ä¸‹é¢å°±è·Ÿå¤§å®¶è§£é‡Šä¸€ä¸‹è¿™äº›ä¸åŒçš„ Attention éƒ½æœ‰å“ªäº›å·®åˆ«ã€‚

        1. è®¡ç®—åŒºåŸŸ

        æ ¹æ®Attentionçš„è®¡ç®—åŒºåŸŸï¼Œå¯ä»¥åˆ†æˆä»¥ä¸‹å‡ ç§ï¼š

        1ï¼‰Soft Attentionï¼Œè¿™æ˜¯æ¯”è¾ƒå¸¸è§çš„Attentionæ–¹å¼ï¼Œå¯¹æ‰€æœ‰keyæ±‚æƒé‡æ¦‚ç‡ï¼Œæ¯ä¸ªkeyéƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„æƒé‡ï¼Œæ˜¯ä¸€ç§å…¨å±€çš„è®¡ç®—æ–¹å¼ï¼ˆä¹Ÿå¯ä»¥å«Global Attentionï¼‰ã€‚è¿™ç§æ–¹å¼æ¯”è¾ƒç†æ€§ï¼Œå‚è€ƒäº†æ‰€æœ‰keyçš„å†…å®¹ï¼Œå†è¿›è¡ŒåŠ æƒã€‚ä½†æ˜¯è®¡ç®—é‡å¯èƒ½ä¼šæ¯”è¾ƒå¤§ä¸€äº›ã€‚

        2ï¼‰Hard Attentionï¼Œè¿™ç§æ–¹å¼æ˜¯ç›´æ¥ç²¾å‡†å®šä½åˆ°æŸä¸ªkeyï¼Œå…¶ä½™keyå°±éƒ½ä¸ç®¡äº†ï¼Œç›¸å½“äºè¿™ä¸ªkeyçš„æ¦‚ç‡æ˜¯1ï¼Œå…¶ä½™keyçš„æ¦‚ç‡å…¨éƒ¨æ˜¯0ã€‚å› æ­¤è¿™ç§å¯¹é½æ–¹å¼è¦æ±‚å¾ˆé«˜ï¼Œè¦æ±‚ä¸€æ­¥åˆ°ä½ï¼Œå¦‚æœæ²¡æœ‰æ­£ç¡®å¯¹é½ï¼Œä¼šå¸¦æ¥å¾ˆå¤§çš„å½±å“ã€‚å¦ä¸€æ–¹é¢ï¼Œå› ä¸ºä¸å¯å¯¼ï¼Œä¸€èˆ¬éœ€è¦ç”¨å¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚ï¼ˆæˆ–è€…ä½¿ç”¨gumbel softmaxä¹‹ç±»çš„ï¼‰

        3ï¼‰Local Attentionï¼Œè¿™ç§æ–¹å¼å…¶å®æ˜¯ä»¥ä¸Šä¸¤ç§æ–¹å¼çš„ä¸€ä¸ªæŠ˜ä¸­ï¼Œå¯¹ä¸€ä¸ªçª—å£åŒºåŸŸè¿›è¡Œè®¡ç®—ã€‚å…ˆç”¨Hardæ–¹å¼å®šä½åˆ°æŸä¸ªåœ°æ–¹ï¼Œä»¥è¿™ä¸ªç‚¹ä¸ºä¸­å¿ƒå¯ä»¥å¾—åˆ°ä¸€ä¸ªçª—å£åŒºåŸŸï¼Œåœ¨è¿™ä¸ªå°åŒºåŸŸå†…ç”¨Softæ–¹å¼æ¥ç®—Attentionã€‚

        2. æ‰€ç”¨ä¿¡æ¯

        å‡è®¾æˆ‘ä»¬è¦å¯¹ä¸€æ®µåŸæ–‡è®¡ç®—Attentionï¼Œè¿™é‡ŒåŸæ–‡æŒ‡çš„æ˜¯æˆ‘ä»¬è¦åšattentionçš„æ–‡æœ¬ï¼Œé‚£ä¹ˆæ‰€ç”¨ä¿¡æ¯åŒ…æ‹¬å†…éƒ¨ä¿¡æ¯å’Œå¤–éƒ¨ä¿¡æ¯ï¼Œå†…éƒ¨ä¿¡æ¯æŒ‡çš„æ˜¯åŸæ–‡æœ¬èº«çš„ä¿¡æ¯ï¼Œè€Œå¤–éƒ¨ä¿¡æ¯æŒ‡çš„æ˜¯é™¤åŸæ–‡ä»¥å¤–çš„é¢å¤–ä¿¡æ¯ã€‚

        1ï¼‰General Attentionï¼Œè¿™ç§æ–¹å¼åˆ©ç”¨åˆ°äº†å¤–éƒ¨ä¿¡æ¯ï¼Œå¸¸ç”¨äºéœ€è¦æ„å»ºä¸¤æ®µæ–‡æœ¬å…³ç³»çš„ä»»åŠ¡ï¼Œqueryä¸€èˆ¬åŒ…å«äº†é¢å¤–ä¿¡æ¯ï¼Œæ ¹æ®å¤–éƒ¨queryå¯¹åŸæ–‡è¿›è¡Œå¯¹é½ã€‚

        æ¯”å¦‚åœ¨é˜…è¯»ç†è§£ä»»åŠ¡ä¸­ï¼Œéœ€è¦æ„å»ºé—®é¢˜å’Œæ–‡ç« çš„å…³è”ï¼Œå‡è®¾ç°åœ¨baselineæ˜¯ï¼Œå¯¹é—®é¢˜è®¡ç®—å‡ºä¸€ä¸ªé—®é¢˜å‘é‡qï¼ŒæŠŠè¿™ä¸ªqå’Œæ‰€æœ‰çš„æ–‡ç« è¯å‘é‡æ‹¼æ¥èµ·æ¥ï¼Œè¾“å…¥åˆ°LSTMä¸­è¿›è¡Œå»ºæ¨¡ã€‚é‚£ä¹ˆåœ¨è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œæ–‡ç« æ‰€æœ‰è¯å‘é‡å…±äº«åŒä¸€ä¸ªé—®é¢˜å‘é‡ï¼Œç°åœ¨æˆ‘ä»¬æƒ³è®©æ–‡ç« æ¯ä¸€æ­¥çš„è¯å‘é‡éƒ½æœ‰ä¸€ä¸ªä¸åŒçš„é—®é¢˜å‘é‡ï¼Œä¹Ÿå°±æ˜¯ï¼Œåœ¨æ¯ä¸€æ­¥ä½¿ç”¨æ–‡ç« åœ¨è¯¥æ­¥ä¸‹çš„è¯å‘é‡å¯¹é—®é¢˜æ¥ç®—attentionï¼Œè¿™é‡Œé—®é¢˜å±äºåŸæ–‡ï¼Œæ–‡ç« è¯å‘é‡å°±å±äºå¤–éƒ¨ä¿¡æ¯ã€‚

        2ï¼‰Local Attentionï¼Œè¿™ç§æ–¹å¼åªä½¿ç”¨å†…éƒ¨ä¿¡æ¯ï¼Œkeyå’Œvalueä»¥åŠqueryåªå’Œè¾“å…¥åŸæ–‡æœ‰å…³ï¼Œåœ¨self attentionä¸­ï¼Œkey=value=queryã€‚æ—¢ç„¶æ²¡æœ‰å¤–éƒ¨ä¿¡æ¯ï¼Œé‚£ä¹ˆåœ¨åŸæ–‡ä¸­çš„æ¯ä¸ªè¯å¯ä»¥è·Ÿè¯¥å¥å­ä¸­çš„æ‰€æœ‰è¯è¿›è¡ŒAttentionè®¡ç®—ï¼Œç›¸å½“äºå¯»æ‰¾åŸæ–‡å†…éƒ¨çš„å…³ç³»ã€‚

        è¿˜æ˜¯ä¸¾é˜…è¯»ç†è§£ä»»åŠ¡çš„ä¾‹å­ï¼Œä¸Šé¢çš„baselineä¸­æåˆ°ï¼Œå¯¹é—®é¢˜è®¡ç®—å‡ºä¸€ä¸ªå‘é‡qï¼Œé‚£ä¹ˆè¿™é‡Œä¹Ÿå¯ä»¥ç”¨ä¸Šattentionï¼Œåªç”¨é—®é¢˜è‡ªèº«çš„ä¿¡æ¯å»åšattentionï¼Œè€Œä¸å¼•å…¥æ–‡ç« ä¿¡æ¯ã€‚

        3. ç»“æ„å±‚æ¬¡

        ç»“æ„æ–¹é¢æ ¹æ®æ˜¯å¦åˆ’åˆ†å±‚æ¬¡å…³ç³»ï¼Œåˆ†ä¸ºå•å±‚attentionï¼Œå¤šå±‚attentionå’Œå¤šå¤´attentionï¼š

        1ï¼‰å•å±‚Attentionï¼Œè¿™æ˜¯æ¯”è¾ƒæ™®éçš„åšæ³•ï¼Œç”¨ä¸€ä¸ªqueryå¯¹ä¸€æ®µåŸæ–‡è¿›è¡Œä¸€æ¬¡attentionã€‚

        2ï¼‰å¤šå±‚Attentionï¼Œä¸€èˆ¬ç”¨äºæ–‡æœ¬å…·æœ‰å±‚æ¬¡å…³ç³»çš„æ¨¡å‹ï¼Œå‡è®¾æˆ‘ä»¬æŠŠä¸€ä¸ªdocumentåˆ’åˆ†æˆå¤šä¸ªå¥å­ï¼Œåœ¨ç¬¬ä¸€å±‚ï¼Œæˆ‘ä»¬åˆ†åˆ«å¯¹æ¯ä¸ªå¥å­ä½¿ç”¨attentionè®¡ç®—å‡ºä¸€ä¸ªå¥å‘é‡ï¼ˆä¹Ÿå°±æ˜¯å•å±‚attentionï¼‰ï¼›åœ¨ç¬¬äºŒå±‚ï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰å¥å‘é‡å†åšattentionè®¡ç®—å‡ºä¸€ä¸ªæ–‡æ¡£å‘é‡ï¼ˆä¹Ÿæ˜¯ä¸€ä¸ªå•å±‚attentionï¼‰ï¼Œæœ€åå†ç”¨è¿™ä¸ªæ–‡æ¡£å‘é‡å»åšä»»åŠ¡ã€‚

        3ï¼‰å¤šå¤´Attentionï¼Œè¿™æ˜¯Attention is All You Needä¸­æåˆ°çš„multi-head attentionï¼Œç”¨åˆ°äº†å¤šä¸ªqueryå¯¹ä¸€æ®µåŸæ–‡è¿›è¡Œäº†å¤šæ¬¡attentionï¼Œæ¯ä¸ªqueryéƒ½å…³æ³¨åˆ°åŸæ–‡çš„ä¸åŒéƒ¨åˆ†ï¼Œç›¸å½“äºé‡å¤åšå¤šæ¬¡å•å±‚attentionï¼š


        æœ€åå†æŠŠè¿™äº›ç»“æœæ‹¼æ¥èµ·æ¥ï¼š


        4. æ¨¡å‹æ–¹é¢

        ä»æ¨¡å‹ä¸Šçœ‹ï¼ŒAttentionä¸€èˆ¬ç”¨åœ¨CNNå’ŒLSTMä¸Šï¼Œä¹Ÿå¯ä»¥ç›´æ¥è¿›è¡Œçº¯Attentionè®¡ç®—ã€‚

        1ï¼‰CNN+Attention

        CNNçš„å·ç§¯æ“ä½œå¯ä»¥æå–é‡è¦ç‰¹å¾ï¼Œæˆ‘è§‰å¾—è¿™ä¹Ÿç®—æ˜¯Attentionçš„æ€æƒ³ï¼Œä½†æ˜¯CNNçš„å·ç§¯æ„Ÿå—è§†é‡æ˜¯å±€éƒ¨çš„ï¼Œéœ€è¦é€šè¿‡å åŠ å¤šå±‚å·ç§¯åŒºå»æ‰©å¤§è§†é‡ã€‚å¦å¤–ï¼ŒMax Poolingç›´æ¥æå–æ•°å€¼æœ€å¤§çš„ç‰¹å¾ï¼Œä¹Ÿåƒæ˜¯hard attentionçš„æ€æƒ³ï¼Œç›´æ¥é€‰ä¸­æŸä¸ªç‰¹å¾ã€‚

        CNNä¸ŠåŠ Attentionå¯ä»¥åŠ åœ¨è¿™å‡ æ–¹é¢ï¼š

        a. åœ¨å·ç§¯æ“ä½œå‰åšattentionï¼Œæ¯”å¦‚Attention-Based BCNN-1ï¼Œè¿™ä¸ªä»»åŠ¡æ˜¯æ–‡æœ¬è•´å«ä»»åŠ¡éœ€è¦å¤„ç†ä¸¤æ®µæ–‡æœ¬ï¼ŒåŒæ—¶å¯¹ä¸¤æ®µè¾“å…¥çš„åºåˆ—å‘é‡è¿›è¡Œattentionï¼Œè®¡ç®—å‡ºç‰¹å¾å‘é‡ï¼Œå†æ‹¼æ¥åˆ°åŸå§‹å‘é‡ä¸­ï¼Œä½œä¸ºå·ç§¯å±‚çš„è¾“å…¥ã€‚

        b. åœ¨å·ç§¯æ“ä½œååšattentionï¼Œæ¯”å¦‚Attention-Based BCNN-2ï¼Œå¯¹ä¸¤æ®µæ–‡æœ¬çš„å·ç§¯å±‚çš„è¾“å‡ºåšattentionï¼Œä½œä¸ºpoolingå±‚çš„è¾“å…¥ã€‚

        c. åœ¨poolingå±‚åšattentionï¼Œä»£æ›¿max poolingã€‚æ¯”å¦‚Attention poolingï¼Œé¦–å…ˆæˆ‘ä»¬ç”¨LSTMå­¦åˆ°ä¸€ä¸ªæ¯”è¾ƒå¥½çš„å¥å‘é‡ï¼Œä½œä¸ºqueryï¼Œç„¶åç”¨CNNå…ˆå­¦ä¹ åˆ°ä¸€ä¸ªç‰¹å¾çŸ©é˜µä½œä¸ºkeyï¼Œå†ç”¨queryå¯¹keyäº§ç”Ÿæƒé‡ï¼Œè¿›è¡Œattentionï¼Œå¾—åˆ°æœ€åçš„å¥å‘é‡ã€‚

        2ï¼‰LSTM+Attention

        LSTMå†…éƒ¨æœ‰Gateæœºåˆ¶ï¼Œå…¶ä¸­input gateé€‰æ‹©å“ªäº›å½“å‰ä¿¡æ¯è¿›è¡Œè¾“å…¥ï¼Œforget gateé€‰æ‹©é—å¿˜å“ªäº›è¿‡å»ä¿¡æ¯ï¼Œæˆ‘è§‰å¾—è¿™ç®—æ˜¯ä¸€å®šç¨‹åº¦çš„Attentionäº†ï¼Œè€Œä¸”å·ç§°å¯ä»¥è§£å†³é•¿æœŸä¾èµ–é—®é¢˜ï¼Œå®é™…ä¸ŠLSTMéœ€è¦ä¸€æ­¥ä¸€æ­¥å»æ•æ‰åºåˆ—ä¿¡æ¯ï¼Œåœ¨é•¿æ–‡æœ¬ä¸Šçš„è¡¨ç°æ˜¯ä¼šéšç€stepå¢åŠ è€Œæ…¢æ…¢è¡°å‡ï¼Œéš¾ä»¥ä¿ç•™å…¨éƒ¨çš„æœ‰ç”¨ä¿¡æ¯ã€‚

        LSTMé€šå¸¸éœ€è¦å¾—åˆ°ä¸€ä¸ªå‘é‡ï¼Œå†å»åšä»»åŠ¡ï¼Œå¸¸ç”¨æ–¹å¼æœ‰ï¼š

        a. ç›´æ¥ä½¿ç”¨æœ€åçš„hidden stateï¼ˆå¯èƒ½ä¼šæŸå¤±ä¸€å®šçš„å‰æ–‡ä¿¡æ¯ï¼Œéš¾ä»¥è¡¨è¾¾å…¨æ–‡ï¼‰

        b. å¯¹æ‰€æœ‰stepä¸‹çš„hidden stateè¿›è¡Œç­‰æƒå¹³å‡ï¼ˆå¯¹æ‰€æœ‰stepä¸€è§†åŒä»ï¼‰ã€‚

        c. Attentionæœºåˆ¶ï¼Œå¯¹æ‰€æœ‰stepçš„hidden stateè¿›è¡ŒåŠ æƒï¼ŒæŠŠæ³¨æ„åŠ›é›†ä¸­åˆ°æ•´æ®µæ–‡æœ¬ä¸­æ¯”è¾ƒé‡è¦çš„hidden stateä¿¡æ¯ã€‚æ€§èƒ½æ¯”å‰é¢ä¸¤ç§è¦å¥½ä¸€ç‚¹ï¼Œè€Œæ–¹ä¾¿å¯è§†åŒ–è§‚å¯Ÿå“ªäº›stepæ˜¯é‡è¦çš„ï¼Œä½†æ˜¯è¦å°å¿ƒè¿‡æ‹Ÿåˆï¼Œè€Œä¸”ä¹Ÿå¢åŠ äº†è®¡ç®—é‡ã€‚

        3ï¼‰çº¯Attention

        Attention is all you needï¼Œæ²¡æœ‰ç”¨åˆ°CNN/RNNï¼Œä¹ä¸€å¬ä¹Ÿæ˜¯ä¸€è‚¡æ¸…æµäº†ï¼Œä½†æ˜¯ä»”ç»†ä¸€çœ‹ï¼Œæœ¬è´¨ä¸Šè¿˜æ˜¯ä¸€å †å‘é‡å»è®¡ç®—attentionã€‚
    """)
    st.image("./data/images/attentiontypes.png")




def st_dl1():
    st.image("./data/images/mlpipeline.png")
    st.markdown("""

        LSTM networks are a type of Recurrent Neural Network (RNN) specially designed to remember and process sequences of data over long periods. 
        What sets LSTMs apart from traditional RNNs is their ability to preserve information for long durations, courtesy of their unique structure comprising three gates: the input, forget, and output gates.
        These gates collaboratively manage the flow of information, deciding what to retain and what to discard, thereby mitigating the issue of vanishing gradients â€” a common problem in standard RNNs.
    
    """)
    
    st.header("Attention Mechanism: Enhancing LSTM")
    st.markdown("""
        The attention mechanism, initially popularized in the field of natural language processing, has found its way into various other domains, including finance. 
        It operates on a simple yet profound concept: not all parts of the input sequence are equally important. 
        By allowing the model to focus on specific parts of the input sequence while ignoring others, the attention mechanism enhances the modelâ€™s context understanding capabilities.

        Incorporating attention into LSTM networks results in a more focused and context-aware model. 
        When predicting stock prices, certain historical data points may be more relevant than others. 
        The attention mechanism empowers the LSTM to weigh these points more heavily, leading to more accurate and nuanced predictions.
    """)
    
    
    

def st_dl2():
    st.image("./data/images/mlpipeline.png")
    st.markdown("""

                """)

def st_dl3():
    st.markdown("""
        In this model, units represent the number of neurons in each LSTM layer. return_sequences=True is crucial in the first layers to ensure the output includes sequences, which are essential for stacking LSTM layers. The final LSTM layer does not return sequences as we prepare the data for the attention layer.                
   
        """)

def st_dl4():
    st.markdown("""
        The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def st_dl4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)
def st_dl4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def st_dl4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def st_dl4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def st_dl4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def st_dl4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def st_dl4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def st_dl4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def st_dl0():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def st_dl4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def st_dl4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def st_dl4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def st_dl4():
    st.markdown("""
The attention mechanism can be added to enhance the modelâ€™s ability to focus on relevant time steps:
                """)

def st_dl6():
    st.markdown("""

Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm_3 (LSTM)               (None, 60, 50)            10400     
                                                                 
 lstm_4 (LSTM)               (None, 60, 50)            20200     
                                                                 
 permute (Permute)           (None, 50, 60)            0         
                                                                 
 reshape (Reshape)           (None, 50, 60)            0         
                                                                 
 permute_1 (Permute)         (None, 60, 50)            0         
                                                                 
 reshape_1 (Reshape)         (None, 60, 50)            0         
                                                                 
 flatten (Flatten)           (None, 3000)              0         
                                                                 
 dense_1 (Dense)             (None, 1)                 3001      
                                                                 
 dropout (Dropout)           (None, 1)                 0         
                                                                 
 batch_normalization (Batch  (None, 1)                 4         
 Normalization)                                                  
                                                                 
=================================================================
Total params: 33605 (131.27 KB)
Trainable params: 33603 (131.26 KB)
Non-trainable params: 2 (8.00 Byte)
_________________________________________________________________


                """)

def st_dl11():
    st.markdown("""
In this guide, we explored the complex yet fascinating task of using LSTM networks with an attention mechanism for stock price prediction, 
specifically for Apple Inc. (AAPL). Key points include:

- LSTMâ€™s ability to capture long-term dependencies in time-series data.
- The added advantage of the attention mechanism in focusing on relevant data points.
- The detailed process of building, training, and evaluating the LSTM model.

#### While LSTM models with attention are powerful, they have limitations:
- The assumption that historical patterns will repeat in similar ways can be problematic, especially in volatile markets.
- External factors like market news and global events, not captured in historical price data, can significantly influence stock prices.
""")

