import streamlit as st, json, csv, pandas as pd, os
from streamlit_extras.add_vertical_space import add_vertical_space
from ragas import evaluate
from datasets import Dataset 
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from ragas.metrics.critique import harmfulness
from langchain_community.vectorstores import Chroma
from langchain_experimental.text_splitter import SemanticChunker
from langchain_community.document_loaders import SeleniumURLLoader
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall, context_entity_recall, answer_similarity, answer_correctness


def news():
    st.subheader("How does one evaluate these RAG pipelines?")
    st.write("""
             
             A RAG pipeline is evaluated based on ground truths. So, the evaluation dataset must contain user queries, ground truths, results generated, and context supplied. Before diving into code, we shall explore all the required metrics.

We can divide this process into two parts:

Retrieval Evaluation: This part evaluates the context that is passed to the LLM. The metrics involved are:
Context Recall: The metric checks if all the relevant answers to the question are present in the context. For the user query: ‚ÄúWho discovered the Galapagos Islands and how?‚Äù A context with high recall will answer both the parts of question ‚Äî Who and How. The answer to both of these questions is present in the ground truth. Hence, this metric utilizes context and ground truth to determine a score between 0 and 1. 1 being the highest recall.
Context Precision: This metric determines whether context that is closest to the ground truth is given high score. The more relevant is the chunk to the ground truth the higher will be the score. Context Precision is determined by ground truth, context, and user query. Higher the score, ranging from 0 to 1, higher the context precision.
Context Entities Recall: This recall determines whether all entities present in the ground truth are also present in the supplied context. For query: ‚ÄúIn which countries are snow leopards found?‚Äù the ground truth mentions 12 countries. If the context contains all the names of these countries, then it would result in high context entity recall.
2. Generation Evaluation: This part evaluates the answer generated by the LLM.

Faithfulness: The metric outputs a score between 0 and 1, determining the extent to which the generated response relies solely on the provided context. The lower the score, the less trustworthy the generated answer is, the less reliance on the supplied context.
Answer Relevance: It measures how relevant and pertinent is the generated answer to the user query. For query: ‚ÄúWhat are the threats to penguin populations?‚Äù an irrelevant answer might focus on location of penguins while a relevant answer would mention the threats to penguins populations.
Answer Similarity: It calculates how semantically similar are the generated answer and the ground truth. In simple terms, how these two are similar conceptually. For query: ‚ÄúIn which countries are snow leopards found?‚Äù the generated answer might mention only a few countries with snow leopards, but it would have a high answer similarity because the answer is conceptually similar to the ground truth. Again, 1 being the highest similarity score and 0 being the lowest.
Answer Correctness: This metric determines how factually correct is the generated output. It utilizes the ground truth and generated answer to determine this score. The higher the better, from 0 to 1. It is not to be confused with faithfulness as an answer can be (factually) correct but can not be faithful if it is not generated using the context.
Answer Harmfulness: This metric simply determines if the output is potentially offensive to an individual, group, or a society. The output is binary, 0 or 1.
We can now dive into the code to perform RAG and determine these metrics using Ragas (RAG Assessment) framework. Ragas is a simple framework that helps evaluate RAG pipelines.
             
             
             """)
    
    nextstep = False
    st.write('Follow the pipeline: üÜïRetrieve ‚û°Ô∏è Chunking')
    results = []
    contexts = []
    queries = [
        "Who discovered the Galapagos Islands and how?",
        "What is Brooklyn‚ÄìBattery Tunnel?",
        "Are Penguins found in the Galapagos Islands?",
        "What is the significance of the Statue of Liberty in New York City?",]

    ground_truths = [
        "The Galapagos Islands were discovered in 1535 by the bishop of Panama, Tom√°s de Berlanga, whose ship had drifted off course while en route to Peru. He named them Las Encantadas (‚ÄúThe Enchanted‚Äù), and in his writings he marveled at the thousands of large gal√°pagos (tortoises) found there. Numerous Spanish voyagers stopped at the islands from the 16th century, and the Galapagos also came to be used by pirates and by whale and seal hunters. ",
        "The Brooklyn-Battery Tunnel (officially known as the Hugh L. Carey Tunnel) is the longest continuous underwater vehicular tunnel in North America and runs underneath Battery Park, connecting the Financial District in Lower Manhattan to Red Hook in Brooklyn.[586]",
        "Penguins live on the galapagos islands side by side with tropical animals.",
        "The Statue of Liberty in New York City holds great significance as a symbol of the United States and its ideals of liberty and peace. It greeted millions of immigrants who arrived in the U.S. by ship in the late 19th and early 20th centuries, representing hope and freedom for those seeking a better life. It has since become an iconic landmark and a global symbol of cultural diversity and freedom.",]

    if not os.path.exists('./data/news_contexts.csv'):
        if st.button('Retrieve News'):
            st.text(" creating ./data/news_contexts.csv.")
            urls = [
                "https://en.wikipedia.org/wiki/New_York_City",
                "https://www.britannica.com/place/Galapagos-Islands",
            ]        
            
            with st.spinner('Scraping news ...'):
                # collect data using selenium url loader
                loader = SeleniumURLLoader(urls=urls)
                documents = loader.load()
                documentList = []
                for doc in documents:
                    d = str(doc.page_content).replace("\\n", " ").replace("\\t"," ").replace("\n", " ").replace("\t", " ")
                    documentList.append(d)
            st.text(documentList[:1])
            
            with st.spinner("embedding, chunking ..."):
                st.write(11)
                embedding_function = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
                st.write(21)
                text_splitter = SemanticChunker(embedding_function)
                st.write(31)
                docs = text_splitter.create_documents(documentList)
                # storing embeddings in a folder
                st.write(123)                
                vector_store = Chroma.from_documents(docs, embedding_function, persist_directory="./data/chroma_db_news")
                # use this to load vector database
                st.write(134)
                vector_store = Chroma(persist_directory="./data/chroma_db_news", embedding_function=embedding_function)
                
                st.write('Great!')
                PROMPT_TEMPLATE = """
                    Go through the context and answer given question strictly based on context. 
                    Context: {context}
                    Question: {question}
                    Answer:
                    """
                    
                qa_chain = RetrievalQA.from_chain_type(
                        llm = ChatOpenAI(temperature=0),
                        # retriever=vector_store.as_retriever(search_kwargs={'k': 3}),
                        retriever=vector_store.as_retriever(),
                        return_source_documents=True,
                        chain_type_kwargs={"prompt": PromptTemplate.from_template(PROMPT_TEMPLATE)})
                
                for query in queries:
                    st.text(query)
                    result = qa_chain({"query": query})
                    st.text(result)
                
                    results.append(result['result'])
                    sources = result["source_documents"]
                    contents = []
                    for i in range(len(sources)):
                        contents.append(sources[i].page_content)
                    contexts.append(contents)
                    
                df = pd.DataFrame(contexts)
                df_results = pd.DataFrame(results)
                df.to_csv('./data/news_contexts.csv', index=False)  # Save without index
                df.to_json("./data/news_contexts.json", orient="records")  # Save each row as a
                df_results.to_csv('./data/news_results.csv', index=False)  # Save without index
                df_results.to_json("./data/news_results.json", orient="records")  # Save each row as a
                
                if 'news' not in st.session_state:   st.session_state['news'] = ''
                st.session_state['news'] = contexts
    else:
        contexts = pd.read_json('./data/news.json')
        results  = pd.read_json('./data/news.json')
        st.text(contexts)
        
        
        st.code("""
        
        d = {
            "question": queries,
            "answer": results,
            "contexts": contexts,
            "ground_truth": ground_truths
        }

        dataset = Dataset.from_dict(d)
        score = evaluate(dataset,metrics=[faithfulness, answer_relevancy, context_precision, context_recall, context_entity_recall, answer_similarity, answer_correctness, harmfulness])
        score_df = score.to_pandas()
        score_df.to_csv("./data/EvaluationScores.csv", encoding="utf-8", index=False)
        st.text(score_df)
        """)
        
        st.code("""
                 
                 faithfulness             0.955000
answer_relevancy         0.923192
context_precision        0.733333
context_recall           0.916667
context_entity_recall    0.322197
answer_similarity        0.941792
answer_correctness       0.665889
harmfulness              0.000000
dtype: float64
                 
                 """)